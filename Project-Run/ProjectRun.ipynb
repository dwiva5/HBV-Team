{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for Run Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from neuralhydrology.evaluation import metrics\n",
    "from neuralhydrology.nh_run import start_run, eval_run\n",
    "from neuralhydrology.nh_run_scheduler import schedule_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model\n",
    "\n",
    "1. Base Model = LSTM without dropout rate\n",
    "2. MCD Model = LSTM with Regression head, Monte Carlo Dropout, and dropout rate\n",
    "3. UMAL Model = LSTM with UMAL head and dropout rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-24 16:39:16,124: Logging to C:\\Users\\Dwiva5\\Downloads\\DSAI\\Project\\hbv-runoff\\Project-Run\\runs\\run_cudalstm_2401_163916\\output.log initialized.\n",
      "2024-01-24 16:39:16,124: ### Folder structure created at C:\\Users\\Dwiva5\\Downloads\\DSAI\\Project\\hbv-runoff\\Project-Run\\runs\\run_cudalstm_2401_163916\n",
      "2024-01-24 16:39:16,124: ### Run configurations for run_cudalstm\n",
      "2024-01-24 16:39:16,125: experiment_name: run_cudalstm\n",
      "2024-01-24 16:39:16,125: train_basin_file: basin_huc_17.txt\n",
      "2024-01-24 16:39:16,125: validation_basin_file: basin_huc_17.txt\n",
      "2024-01-24 16:39:16,126: test_basin_file: basin_huc_17.txt\n",
      "2024-01-24 16:39:16,126: train_start_date: 1980-01-01 00:00:00\n",
      "2024-01-24 16:39:16,127: train_end_date: 1999-12-31 00:00:00\n",
      "2024-01-24 16:39:16,127: validation_start_date: 2000-01-01 00:00:00\n",
      "2024-01-24 16:39:16,128: validation_end_date: 2004-12-31 00:00:00\n",
      "2024-01-24 16:39:16,128: test_start_date: 2005-01-01 00:00:00\n",
      "2024-01-24 16:39:16,128: test_end_date: 2009-12-31 00:00:00\n",
      "2024-01-24 16:39:16,129: device: cuda:0\n",
      "2024-01-24 16:39:16,129: validate_every: 3\n",
      "2024-01-24 16:39:16,129: validate_n_random_basins: 91\n",
      "2024-01-24 16:39:16,130: metrics: ['NSE']\n",
      "2024-01-24 16:39:16,130: model: cudalstm\n",
      "2024-01-24 16:39:16,131: head: regression\n",
      "2024-01-24 16:39:16,131: output_activation: linear\n",
      "2024-01-24 16:39:16,131: hidden_size: 20\n",
      "2024-01-24 16:39:16,132: initial_forget_bias: 3\n",
      "2024-01-24 16:39:16,132: output_dropout: 0\n",
      "2024-01-24 16:39:16,133: optimizer: Adam\n",
      "2024-01-24 16:39:16,133: loss: MSE\n",
      "2024-01-24 16:39:16,133: learning_rate: {0: 0.01, 30: 0.005, 40: 0.001}\n",
      "2024-01-24 16:39:16,134: batch_size: 256\n",
      "2024-01-24 16:39:16,134: epochs: 50\n",
      "2024-01-24 16:39:16,134: clip_gradient_norm: 1\n",
      "2024-01-24 16:39:16,135: predict_last_n: 1\n",
      "2024-01-24 16:39:16,135: seq_length: 365\n",
      "2024-01-24 16:39:16,136: num_workers: 8\n",
      "2024-01-24 16:39:16,136: log_interval: 5\n",
      "2024-01-24 16:39:16,136: log_tensorboard: True\n",
      "2024-01-24 16:39:16,137: log_n_figures: 1\n",
      "2024-01-24 16:39:16,137: save_weights_every: 1\n",
      "2024-01-24 16:39:16,137: dataset: camels_us\n",
      "2024-01-24 16:39:16,138: data_dir: ..\\data\\CAMELS_US\n",
      "2024-01-24 16:39:16,138: forcings: ['maurer', 'daymet', 'nldas']\n",
      "2024-01-24 16:39:16,139: dynamic_inputs: ['PRCP(mm/day)_nldas', 'PRCP(mm/day)_maurer', 'prcp(mm/day)_daymet', 'srad(W/m2)_daymet', 'tmax(C)_daymet', 'tmin(C)_daymet', 'vp(Pa)_daymet']\n",
      "2024-01-24 16:39:16,139: target_variables: ['QObs(mm/d)']\n",
      "2024-01-24 16:39:16,139: clip_targets_to_zero: ['QObs(mm/d)']\n",
      "2024-01-24 16:39:16,140: static_attributes: ['elev_mean', 'slope_mean', 'area_gages2', 'frac_forest', 'lai_max', 'lai_diff', 'gvf_max', 'gvf_diff', 'soil_depth_pelletier', 'soil_depth_statsgo', 'soil_porosity', 'soil_conductivity', 'max_water_content', 'sand_frac', 'silt_frac', 'clay_frac', 'carbonate_rocks_frac', 'geol_permeability', 'p_mean', 'pet_mean', 'aridity', 'frac_snow', 'high_prec_freq', 'high_prec_dur', 'low_prec_freq', 'low_prec_dur']\n",
      "2024-01-24 16:39:16,140: number_of_basins: 91\n",
      "2024-01-24 16:39:16,140: run_dir: C:\\Users\\Dwiva5\\Downloads\\DSAI\\Project\\hbv-runoff\\Project-Run\\runs\\run_cudalstm_2401_163916\n",
      "2024-01-24 16:39:16,141: train_dir: C:\\Users\\Dwiva5\\Downloads\\DSAI\\Project\\hbv-runoff\\Project-Run\\runs\\run_cudalstm_2401_163916\\train_data\n",
      "2024-01-24 16:39:16,141: img_log_dir: C:\\Users\\Dwiva5\\Downloads\\DSAI\\Project\\hbv-runoff\\Project-Run\\runs\\run_cudalstm_2401_163916\\img_log\n",
      "2024-01-24 16:39:16,152: ### Device cuda:0 will be used for training\n",
      "2024-01-24 16:39:16,277: Loading basin data into xarray data set.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 91/91 [00:18<00:00,  4.85it/s]\n",
      "2024-01-24 16:39:35,184: Create lookup table and convert to pytorch tensor\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 91/91 [00:06<00:00, 13.61it/s]\n",
      "# Epoch 1: 100%|█████████████████████████████████████████████████████| 2382/2382 [01:11<00:00, 33.37it/s, Loss: 0.0524]\n",
      "2024-01-24 16:40:54,139: Epoch 1 average loss: avg_loss: 0.09030, avg_total_loss: 0.09030\n",
      "# Epoch 2: 100%|█████████████████████████████████████████████████████| 2382/2382 [01:09<00:00, 34.51it/s, Loss: 0.0603]\n",
      "2024-01-24 16:42:03,179: Epoch 2 average loss: avg_loss: 0.07137, avg_total_loss: 0.07137\n",
      "# Epoch 3:   0%|                                                                              | 0/2382 [00:23<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# by default we assume that you have at least one CUDA-capable NVIDIA GPU\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mstart_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbase_model_cudalstm.yml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# fall back to CPU-only mode\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m     start_run(config_file\u001b[38;5;241m=\u001b[39mPath(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase_model_cudalstm.yml\u001b[39m\u001b[38;5;124m\"\u001b[39m), gpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuralhydrology\\lib\\site-packages\\neuralhydrology\\nh_run.py:76\u001b[0m, in \u001b[0;36mstart_run\u001b[1;34m(config_file, gpu)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gpu \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gpu \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     74\u001b[0m     config\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 76\u001b[0m \u001b[43mstart_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuralhydrology\\lib\\site-packages\\neuralhydrology\\training\\train.py:20\u001b[0m, in \u001b[0;36mstart_training\u001b[1;34m(cfg)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown head \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;241m.\u001b[39mhead\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m trainer\u001b[38;5;241m.\u001b[39minitialize_training()\n\u001b[1;32m---> 20\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_and_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuralhydrology\\lib\\site-packages\\neuralhydrology\\training\\basetrainer.py:215\u001b[0m, in \u001b[0;36mBaseTrainer.train_and_validate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m param_group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[0;32m    213\u001b[0m         param_group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mlearning_rate[epoch]\n\u001b[1;32m--> 215\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m avg_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperiment_logger\u001b[38;5;241m.\u001b[39msummarise()\n\u001b[0;32m    217\u001b[0m loss_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m avg_losses\u001b[38;5;241m.\u001b[39mitems())\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuralhydrology\\lib\\site-packages\\neuralhydrology\\training\\basetrainer.py:285\u001b[0m, in \u001b[0;36mBaseTrainer._train_epoch\u001b[1;34m(self, epoch)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# Iterate in batches over training set\u001b[39;00m\n\u001b[0;32m    284\u001b[0m nan_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 285\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pbar):\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_updates_per_epoch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_updates_per_epoch:\n\u001b[0;32m    287\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuralhydrology\\lib\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuralhydrology\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:438\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuralhydrology\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:386\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuralhydrology\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1039\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1032\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1033\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1039\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuralhydrology\\lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuralhydrology\\lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuralhydrology\\lib\\multiprocessing\\context.py:336\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuralhydrology\\lib\\multiprocessing\\popen_spawn_win32.py:93\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 93\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuralhydrology\\lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# by default we assume that you have at least one CUDA-capable NVIDIA GPU\n",
    "if torch.cuda.is_available():\n",
    "    start_run(config_file=Path(\"base_model_cudalstm.yml\"))\n",
    "\n",
    "# fall back to CPU-only mode\n",
    "else:\n",
    "    start_run(config_file=Path(\"base_model_cudalstm.yml\"), gpu=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCD Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by default we assume that you have at least one CUDA-capable NVIDIA GPU\n",
    "if torch.cuda.is_available():\n",
    "    start_run(config_file=Path(\"mcd_model_cudalstm.yml\"))\n",
    "\n",
    "# fall back to CPU-only mode\n",
    "else:\n",
    "    start_run(config_file=Path(\"mcd_model_cudalstm.yml\"), gpu=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-24 21:14:50,662: Logging to C:\\Users\\Dwiva5\\Downloads\\DSAI\\Project\\hbv-runoff\\Project-Run\\runs\\run_cudalstm_2401_211450\\output.log initialized.\n",
      "2024-01-24 21:14:50,662: ### Folder structure created at C:\\Users\\Dwiva5\\Downloads\\DSAI\\Project\\hbv-runoff\\Project-Run\\runs\\run_cudalstm_2401_211450\n",
      "2024-01-24 21:14:50,663: ### Run configurations for run_cudalstm\n",
      "2024-01-24 21:14:50,663: experiment_name: run_cudalstm\n",
      "2024-01-24 21:14:50,664: train_basin_file: basin_huc_17.txt\n",
      "2024-01-24 21:14:50,664: validation_basin_file: basin_huc_17.txt\n",
      "2024-01-24 21:14:50,664: test_basin_file: basin_huc_17.txt\n",
      "2024-01-24 21:14:50,665: train_start_date: 1980-01-01 00:00:00\n",
      "2024-01-24 21:14:50,665: train_end_date: 1999-12-31 00:00:00\n",
      "2024-01-24 21:14:50,667: validation_start_date: 2000-01-01 00:00:00\n",
      "2024-01-24 21:14:50,667: validation_end_date: 2004-12-31 00:00:00\n",
      "2024-01-24 21:14:50,668: test_start_date: 2005-01-01 00:00:00\n",
      "2024-01-24 21:14:50,668: test_end_date: 2009-12-31 00:00:00\n",
      "2024-01-24 21:14:50,668: device: cuda:0\n",
      "2024-01-24 21:14:50,669: validate_every: 3\n",
      "2024-01-24 21:14:50,669: validate_n_random_basins: 91\n",
      "2024-01-24 21:14:50,669: metrics: ['NSE']\n",
      "2024-01-24 21:14:50,670: model: cudalstm\n",
      "2024-01-24 21:14:50,670: head: umal\n",
      "2024-01-24 21:14:50,671: output_activation: linear\n",
      "2024-01-24 21:14:50,671: hidden_size: 20\n",
      "2024-01-24 21:14:50,671: n_taus: 1\n",
      "2024-01-24 21:14:50,672: umal_extend_batch: True\n",
      "2024-01-24 21:14:50,672: tau_down: 0.1\n",
      "2024-01-24 21:14:50,672: tau_up: 0.9\n",
      "2024-01-24 21:14:50,673: n_samples: 20\n",
      "2024-01-24 21:14:50,673: negative_sample_handling: clip\n",
      "2024-01-24 21:14:50,674: mc_dropout: False\n",
      "2024-01-24 21:14:50,674: output_dropout: 0.4\n",
      "2024-01-24 21:14:50,674: optimizer: Adam\n",
      "2024-01-24 21:14:50,675: loss: UMALLoss\n",
      "2024-01-24 21:14:50,675: learning_rate: {0: 0.005, 30: 0.005, 40: 0.001}\n",
      "2024-01-24 21:14:50,675: batch_size: 256\n",
      "2024-01-24 21:14:50,676: epochs: 50\n",
      "2024-01-24 21:14:50,676: clip_gradient_norm: 1\n",
      "2024-01-24 21:14:50,677: predict_last_n: 1\n",
      "2024-01-24 21:14:50,677: seq_length: 365\n",
      "2024-01-24 21:14:50,677: num_workers: 8\n",
      "2024-01-24 21:14:50,678: log_interval: 5\n",
      "2024-01-24 21:14:50,679: log_tensorboard: True\n",
      "2024-01-24 21:14:50,679: log_n_figures: 1\n",
      "2024-01-24 21:14:50,679: save_weights_every: 1\n",
      "2024-01-24 21:14:50,680: dataset: camels_us\n",
      "2024-01-24 21:14:50,680: data_dir: ..\\data\\CAMELS_US\n",
      "2024-01-24 21:14:50,681: forcings: ['maurer', 'daymet', 'nldas']\n",
      "2024-01-24 21:14:50,681: dynamic_inputs: ['PRCP(mm/day)_nldas', 'PRCP(mm/day)_maurer', 'prcp(mm/day)_daymet', 'srad(W/m2)_daymet', 'tmax(C)_daymet', 'tmin(C)_daymet', 'vp(Pa)_daymet']\n",
      "2024-01-24 21:14:50,681: target_variables: ['QObs(mm/d)']\n",
      "2024-01-24 21:14:50,681: clip_targets_to_zero: ['QObs(mm/d)']\n",
      "2024-01-24 21:14:50,683: static_attributes: ['elev_mean', 'slope_mean', 'area_gages2', 'frac_forest', 'lai_max', 'lai_diff', 'gvf_max', 'gvf_diff', 'soil_depth_pelletier', 'soil_depth_statsgo', 'soil_porosity', 'soil_conductivity', 'max_water_content', 'sand_frac', 'silt_frac', 'clay_frac', 'carbonate_rocks_frac', 'geol_permeability', 'p_mean', 'pet_mean', 'aridity', 'frac_snow', 'high_prec_freq', 'high_prec_dur', 'low_prec_freq', 'low_prec_dur']\n",
      "2024-01-24 21:14:50,683: number_of_basins: 91\n",
      "2024-01-24 21:14:50,684: run_dir: C:\\Users\\Dwiva5\\Downloads\\DSAI\\Project\\hbv-runoff\\Project-Run\\runs\\run_cudalstm_2401_211450\n",
      "2024-01-24 21:14:50,684: train_dir: C:\\Users\\Dwiva5\\Downloads\\DSAI\\Project\\hbv-runoff\\Project-Run\\runs\\run_cudalstm_2401_211450\\train_data\n",
      "2024-01-24 21:14:50,684: img_log_dir: C:\\Users\\Dwiva5\\Downloads\\DSAI\\Project\\hbv-runoff\\Project-Run\\runs\\run_cudalstm_2401_211450\\img_log\n",
      "2024-01-24 21:14:50,693: ### Device cuda:0 will be used for training\n",
      "2024-01-24 21:14:50,806: Loading basin data into xarray data set.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 91/91 [00:22<00:00,  4.04it/s]\n",
      "2024-01-24 21:15:13,470: Create lookup table and convert to pytorch tensor\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 91/91 [00:07<00:00, 11.46it/s]\n",
      "# Epoch 1: 100%|████████████████████████████████████████████████████| 2382/2382 [01:36<00:00, 24.69it/s, Loss: -0.9219]\n",
      "2024-01-24 21:16:58,954: Epoch 1 average loss: avg_loss: -0.47037, avg_total_loss: -0.47037\n",
      "# Epoch 2: 100%|████████████████████████████████████████████████████| 2382/2382 [01:35<00:00, 24.92it/s, Loss: -0.7107]\n",
      "2024-01-24 21:18:34,560: Epoch 2 average loss: avg_loss: -0.81557, avg_total_loss: -0.81557\n",
      "# Epoch 3: 100%|████████████████████████████████████████████████████| 2382/2382 [01:34<00:00, 25.07it/s, Loss: -0.8966]\n",
      "2024-01-24 21:20:09,566: Epoch 3 average loss: avg_loss: -0.92937, avg_total_loss: -0.92937\n",
      "# Validation: 100%|████████████████████████████████████████████████████████████████████| 91/91 [02:11<00:00,  1.44s/it]\n",
      "2024-01-24 21:22:21,384: Epoch 3 average validation loss: 0.47930 -- Median validation metrics: avg_loss: 0.47930, NSE: 0.57230\n",
      "# Epoch 4: 100%|████████████████████████████████████████████████████| 2382/2382 [01:37<00:00, 24.39it/s, Loss: -0.9791]\n",
      "2024-01-24 21:23:59,038: Epoch 4 average loss: avg_loss: -0.98399, avg_total_loss: -0.98399\n",
      "# Epoch 5: 100%|████████████████████████████████████████████████████| 2382/2382 [01:36<00:00, 24.61it/s, Loss: -1.1070]\n",
      "2024-01-24 21:25:35,820: Epoch 5 average loss: avg_loss: -1.01617, avg_total_loss: -1.01617\n",
      "# Epoch 6: 100%|████████████████████████████████████████████████████| 2382/2382 [01:36<00:00, 24.76it/s, Loss: -0.9886]\n",
      "2024-01-24 21:27:12,022: Epoch 6 average loss: avg_loss: -1.04385, avg_total_loss: -1.04385\n",
      "# Validation: 100%|████████████████████████████████████████████████████████████████████| 91/91 [01:46<00:00,  1.17s/it]\n",
      "2024-01-24 21:28:59,238: Epoch 6 average validation loss: 0.26849 -- Median validation metrics: avg_loss: 0.26849, NSE: 0.55679\n",
      "# Epoch 7: 100%|████████████████████████████████████████████████████| 2382/2382 [01:40<00:00, 23.64it/s, Loss: -1.2345]\n",
      "2024-01-24 21:30:40,011: Epoch 7 average loss: avg_loss: -1.06677, avg_total_loss: -1.06677\n",
      "# Epoch 8: 100%|████████████████████████████████████████████████████| 2382/2382 [01:37<00:00, 24.51it/s, Loss: -1.2125]\n",
      "2024-01-24 21:32:17,201: Epoch 8 average loss: avg_loss: -1.08273, avg_total_loss: -1.08273\n",
      "# Epoch 9: 100%|████████████████████████████████████████████████████| 2382/2382 [01:38<00:00, 24.06it/s, Loss: -1.3556]\n",
      "2024-01-24 21:33:56,201: Epoch 9 average loss: avg_loss: -1.08886, avg_total_loss: -1.08886\n",
      "# Validation: 100%|████████████████████████████████████████████████████████████████████| 91/91 [01:44<00:00,  1.15s/it]\n",
      "2024-01-24 21:35:41,302: Epoch 9 average validation loss: 0.57802 -- Median validation metrics: avg_loss: 0.57802, NSE: 0.48219\n",
      "# Epoch 10: 100%|███████████████████████████████████████████████████| 2382/2382 [01:38<00:00, 24.14it/s, Loss: -1.1137]\n",
      "2024-01-24 21:37:20,002: Epoch 10 average loss: avg_loss: -1.10671, avg_total_loss: -1.10671\n",
      "# Epoch 11: 100%|███████████████████████████████████████████████████| 2382/2382 [01:38<00:00, 24.12it/s, Loss: -1.0565]\n",
      "2024-01-24 21:38:58,754: Epoch 11 average loss: avg_loss: -1.11177, avg_total_loss: -1.11177\n",
      "# Epoch 12: 100%|███████████████████████████████████████████████████| 2382/2382 [01:36<00:00, 24.67it/s, Loss: -1.2027]\n",
      "2024-01-24 21:40:35,337: Epoch 12 average loss: avg_loss: -1.13347, avg_total_loss: -1.13347\n",
      "# Validation: 100%|████████████████████████████████████████████████████████████████████| 91/91 [01:48<00:00,  1.19s/it]\n",
      "2024-01-24 21:42:24,218: Epoch 12 average validation loss: 0.77034 -- Median validation metrics: avg_loss: 0.77034, NSE: 0.47258\n",
      "# Epoch 13: 100%|███████████████████████████████████████████████████| 2382/2382 [01:38<00:00, 24.29it/s, Loss: -1.1329]\n",
      "2024-01-24 21:44:02,277: Epoch 13 average loss: avg_loss: -1.14014, avg_total_loss: -1.14014\n",
      "# Epoch 14: 100%|███████████████████████████████████████████████████| 2382/2382 [01:39<00:00, 23.99it/s, Loss: -0.8923]\n",
      "2024-01-24 21:45:41,570: Epoch 14 average loss: avg_loss: -1.14227, avg_total_loss: -1.14227\n",
      "# Epoch 15: 100%|███████████████████████████████████████████████████| 2382/2382 [01:39<00:00, 23.85it/s, Loss: -0.7670]\n",
      "2024-01-24 21:47:21,441: Epoch 15 average loss: avg_loss: -1.15659, avg_total_loss: -1.15659\n",
      "# Validation: 100%|████████████████████████████████████████████████████████████████████| 91/91 [01:46<00:00,  1.17s/it]\n",
      "2024-01-24 21:49:08,115: Epoch 15 average validation loss: 0.83783 -- Median validation metrics: avg_loss: 0.83783, NSE: 0.47614\n",
      "# Epoch 16: 100%|███████████████████████████████████████████████████| 2382/2382 [01:38<00:00, 24.13it/s, Loss: -1.0973]\n",
      "2024-01-24 21:50:46,830: Epoch 16 average loss: avg_loss: -1.16653, avg_total_loss: -1.16653\n",
      "# Epoch 17: 100%|███████████████████████████████████████████████████| 2382/2382 [01:37<00:00, 24.47it/s, Loss: -0.8541]\n",
      "2024-01-24 21:52:24,194: Epoch 17 average loss: avg_loss: -1.16940, avg_total_loss: -1.16940\n",
      "# Epoch 18: 100%|███████████████████████████████████████████████████| 2382/2382 [01:34<00:00, 25.09it/s, Loss: -1.1421]\n",
      "2024-01-24 21:53:59,143: Epoch 18 average loss: avg_loss: -1.17283, avg_total_loss: -1.17283\n",
      "# Validation: 100%|████████████████████████████████████████████████████████████████████| 91/91 [01:43<00:00,  1.13s/it]\n",
      "2024-01-24 21:55:42,565: Epoch 18 average validation loss: 1.03492 -- Median validation metrics: avg_loss: 1.03492, NSE: 0.42842\n",
      "# Epoch 19: 100%|███████████████████████████████████████████████████| 2382/2382 [01:33<00:00, 25.47it/s, Loss: -1.0946]\n",
      "2024-01-24 21:57:16,076: Epoch 19 average loss: avg_loss: -1.17792, avg_total_loss: -1.17792\n",
      "# Epoch 20: 100%|███████████████████████████████████████████████████| 2382/2382 [01:34<00:00, 25.26it/s, Loss: -1.3565]\n",
      "2024-01-24 21:58:50,404: Epoch 20 average loss: avg_loss: -1.18197, avg_total_loss: -1.18197\n",
      "# Epoch 21: 100%|███████████████████████████████████████████████████| 2382/2382 [01:34<00:00, 25.29it/s, Loss: -1.1325]\n",
      "2024-01-24 22:00:24,592: Epoch 21 average loss: avg_loss: -1.18906, avg_total_loss: -1.18906\n",
      "# Validation: 100%|████████████████████████████████████████████████████████████████████| 91/91 [01:44<00:00,  1.14s/it]\n",
      "2024-01-24 22:02:09,188: Epoch 21 average validation loss: 1.34969 -- Median validation metrics: avg_loss: 1.34969, NSE: 0.38070\n",
      "# Epoch 22: 100%|███████████████████████████████████████████████████| 2382/2382 [01:33<00:00, 25.52it/s, Loss: -0.9706]\n",
      "2024-01-24 22:03:42,525: Epoch 22 average loss: avg_loss: -1.19387, avg_total_loss: -1.19387\n",
      "# Epoch 23: 100%|███████████████████████████████████████████████████| 2382/2382 [01:32<00:00, 25.80it/s, Loss: -1.0707]\n",
      "2024-01-24 22:05:14,872: Epoch 23 average loss: avg_loss: -1.19653, avg_total_loss: -1.19653\n",
      "# Epoch 24: 100%|███████████████████████████████████████████████████| 2382/2382 [01:37<00:00, 24.49it/s, Loss: -1.4264]\n",
      "2024-01-24 22:06:52,136: Epoch 24 average loss: avg_loss: -1.20334, avg_total_loss: -1.20334\n",
      "# Validation: 100%|████████████████████████████████████████████████████████████████████| 91/91 [01:40<00:00,  1.11s/it]\n",
      "2024-01-24 22:08:33,281: Epoch 24 average validation loss: 0.99282 -- Median validation metrics: avg_loss: 0.99282, NSE: 0.40897\n",
      "# Epoch 25: 100%|███████████████████████████████████████████████████| 2382/2382 [01:26<00:00, 27.44it/s, Loss: -1.5880]\n",
      "2024-01-24 22:10:00,104: Epoch 25 average loss: avg_loss: -1.20325, avg_total_loss: -1.20325\n",
      "# Epoch 26: 100%|███████████████████████████████████████████████████| 2382/2382 [01:21<00:00, 29.13it/s, Loss: -1.3792]\n",
      "2024-01-24 22:11:21,896: Epoch 26 average loss: avg_loss: -1.20740, avg_total_loss: -1.20740\n",
      "# Epoch 27: 100%|███████████████████████████████████████████████████| 2382/2382 [01:20<00:00, 29.62it/s, Loss: -0.9928]\n",
      "2024-01-24 22:12:42,312: Epoch 27 average loss: avg_loss: -1.21441, avg_total_loss: -1.21441\n",
      "# Validation: 100%|████████████████████████████████████████████████████████████████████| 91/91 [01:26<00:00,  1.05it/s]\n",
      "2024-01-24 22:14:09,251: Epoch 27 average validation loss: 1.24034 -- Median validation metrics: avg_loss: 1.24034, NSE: 0.44236\n",
      "# Epoch 28: 100%|███████████████████████████████████████████████████| 2382/2382 [01:24<00:00, 28.32it/s, Loss: -1.1558]\n",
      "2024-01-24 22:15:33,361: Epoch 28 average loss: avg_loss: -1.21395, avg_total_loss: -1.21395\n",
      "# Epoch 29: 100%|███████████████████████████████████████████████████| 2382/2382 [01:22<00:00, 28.82it/s, Loss: -1.2283]\n",
      "2024-01-24 22:16:56,020: Epoch 29 average loss: avg_loss: -1.15962, avg_total_loss: -1.15962\n",
      "2024-01-24 22:16:56,025: Setting learning rate to 0.005\n",
      "# Epoch 30: 100%|███████████████████████████████████████████████████| 2382/2382 [01:20<00:00, 29.41it/s, Loss: -1.2943]\n",
      "2024-01-24 22:18:17,009: Epoch 30 average loss: avg_loss: -1.21714, avg_total_loss: -1.21714\n",
      "# Validation: 100%|████████████████████████████████████████████████████████████████████| 91/91 [01:26<00:00,  1.05it/s]\n",
      "2024-01-24 22:19:44,147: Epoch 30 average validation loss: 0.71300 -- Median validation metrics: avg_loss: 0.71300, NSE: 0.44455\n",
      "# Epoch 31: 100%|███████████████████████████████████████████████████| 2382/2382 [01:21<00:00, 29.09it/s, Loss: -1.2077]\n",
      "2024-01-24 22:21:06,043: Epoch 31 average loss: avg_loss: -1.21933, avg_total_loss: -1.21933\n",
      "# Epoch 32: 100%|███████████████████████████████████████████████████| 2382/2382 [01:21<00:00, 29.19it/s, Loss: -1.1426]\n",
      "2024-01-24 22:22:27,650: Epoch 32 average loss: avg_loss: -1.22461, avg_total_loss: -1.22461\n",
      "# Epoch 33: 100%|███████████████████████████████████████████████████| 2382/2382 [01:22<00:00, 28.72it/s, Loss: -0.6050]\n",
      "2024-01-24 22:23:50,596: Epoch 33 average loss: avg_loss: -0.88783, avg_total_loss: -0.88783\n",
      "# Validation: 100%|████████████████████████████████████████████████████████████████████| 91/91 [01:27<00:00,  1.04it/s]\n",
      "2024-01-24 22:25:18,049: Epoch 33 average validation loss: 0.75652 -- Median validation metrics: avg_loss: 0.75652, NSE: 0.22795\n",
      "# Epoch 34:  38%|███████████████████▉                                 | 894/2382 [01:00<01:40, 14.87it/s, Loss: 2.4407]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Loss was NaN for 1 times in a row. Stopped training.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# by default we assume that you have at least one CUDA-capable NVIDIA GPU\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mstart_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mumal_model_cudalstm.yml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# fall back to CPU-only mode\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m     start_run(config_file\u001b[38;5;241m=\u001b[39mPath(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mumal_model_cudalstm.yml\u001b[39m\u001b[38;5;124m\"\u001b[39m), gpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuralhydrology\\lib\\site-packages\\neuralhydrology\\nh_run.py:76\u001b[0m, in \u001b[0;36mstart_run\u001b[1;34m(config_file, gpu)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gpu \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gpu \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     74\u001b[0m     config\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 76\u001b[0m \u001b[43mstart_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuralhydrology\\lib\\site-packages\\neuralhydrology\\training\\train.py:20\u001b[0m, in \u001b[0;36mstart_training\u001b[1;34m(cfg)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown head \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;241m.\u001b[39mhead\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m trainer\u001b[38;5;241m.\u001b[39minitialize_training()\n\u001b[1;32m---> 20\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_and_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuralhydrology\\lib\\site-packages\\neuralhydrology\\training\\basetrainer.py:215\u001b[0m, in \u001b[0;36mBaseTrainer.train_and_validate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m param_group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[0;32m    213\u001b[0m         param_group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mlearning_rate[epoch]\n\u001b[1;32m--> 215\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m avg_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperiment_logger\u001b[38;5;241m.\u001b[39msummarise()\n\u001b[0;32m    217\u001b[0m loss_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m avg_losses\u001b[38;5;241m.\u001b[39mitems())\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\neuralhydrology\\lib\\site-packages\\neuralhydrology\\training\\basetrainer.py:311\u001b[0m, in \u001b[0;36mBaseTrainer._train_epoch\u001b[1;34m(self, epoch)\u001b[0m\n\u001b[0;32m    309\u001b[0m     nan_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nan_count \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_subsequent_nan_losses:\n\u001b[1;32m--> 311\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss was NaN for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnan_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m times in a row. Stopped training.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    312\u001b[0m     LOGGER\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss is Nan; ignoring step. (#\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnan_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_subsequent_nan_losses\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Loss was NaN for 1 times in a row. Stopped training."
     ]
    }
   ],
   "source": [
    "# by default we assume that you have at least one CUDA-capable NVIDIA GPU\n",
    "if torch.cuda.is_available():\n",
    "    start_run(config_file=Path(\"umal_model_cudalstm.yml\"))\n",
    "\n",
    "# fall back to CPU-only mode\n",
    "else:\n",
    "    start_run(config_file=Path(\"umal_model_cudalstm.yml\"), gpu=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate run on test set\n",
    "The run directory that needs to be specified for evaluation is printed in the output log above. Since the folder name is created dynamically (including the date and time of the start of the run) you will need to change the `run_dir` argument according to your local directory name. By default, it will use the same device as during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dir = Path(\"runs/test_run_1601_104731\")\n",
    "eval_run(run_dir=run_dir, period=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and inspect model predictions\n",
    "Next, we load the results file and compare the model predictions with observations. The results file is always a pickled dictionary with one key per basin (even for a single basin). The next-lower dictionary level is the temporal resolution of the predictions. In this case, we trained a model only on daily data ('1D'). Within the temporal resolution, the next-lower dictionary level are `xr`(an xarray Dataset that contains observations and predictions), as well as one key for each metric that was specified in the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(run_dir / \"test\" / \"model_epoch050\" / \"test_results.p\", \"rb\") as fp:\n",
    "    results = pickle.load(fp)\n",
    "    \n",
    "results.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data variables in the xarray Dataset are named according to the name of the target variables, with suffix `_obs` for the observations and suffix `_sim` for the simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['10258000']['1D']['xr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the model predictions vs. the observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract observations and simulations\n",
    "qobs = results['10258000']['1D']['xr']['QObs(mm/d)_obs']\n",
    "qsim = results['10258000']['1D']['xr']['QObs(mm/d)_sim']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,10))\n",
    "ax.plot(qobs['date'], qobs)\n",
    "#ax.plot(qsim['date'], qsim)\n",
    "ax.set_ylabel(\"Discharge (mm/d)\")\n",
    "ax.set_title(f\"Test period - NSE {results['10258000']['1D']['NSE']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralhydrology.evaluation.plots import percentile_plot, regression_plot, uncertainty_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qobstest = qobs.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract observations and simulations\n",
    "qobs = results['10258000']['1D']['xr']['QObs(mm/d)_obs']\n",
    "qsim = results['10258000']['1D']['xr']['QObs(mm/d)_sim']\n",
    "\n",
    "# Plot observations\n",
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "ax.plot(qobs['date'], qobs, label='Observations')\n",
    "\n",
    "# Plot all simulation samples\n",
    "for i in range(qsim.shape[2]):\n",
    "    ax.plot(qsim['date'], qsim.isel(samples=i), color='orange', alpha=0.2, label='_nolegend_')  # Plot each sample with low alpha for transparency\n",
    "\n",
    "ax.set_ylabel(\"Discharge (mm/d)\")\n",
    "ax.set_title(f\"Test period - NSE {results['10258000']['1D']['NSE']:.3f}\")\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_plot(qobstest, qsim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to compute all metrics that are implemented in the NeuralHydrology package. You will find additional hydrological signatures implemented in `neuralhydrology.evaluation.signatures`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = metrics.calculate_all_metrics(qobs.isel(time_step=-1), qsim.isel(time_step=-1))\n",
    "for key, val in values.items():\n",
    "    print(f\"{key}: {val:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
